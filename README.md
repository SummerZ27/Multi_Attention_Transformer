# Multi Attention Transformer

A compact PyTorch implementation of a multi-headed self-attention time-series forecaster, including a Project-Then-Attend (PTA) variant and baselines.

## Contents
- pta_transformer.py: PTA and vanilla Transformer modules
- Naive_Transformer.py: simple transformer baseline
- model_comparison.py: AAPL experiment
- model_comparison_sp500.py: S&P 500 proxy experiment

## Quickstart



## License
MIT
Run experiments:
- python model_comparison.py
- python model_comparison_sp500.py
